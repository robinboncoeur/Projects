# Blog Posts 2025

<style>
  .flex-container {display: flex; gap: 20px; align-items: flex-start;}
  .column {flex: 1 1 0; min-width: 0;}
  .column--right {border-left: 1px solid var(--md-default-fg-color--lightest); padding-left: 20px; }
</style>

**INDEX**

[Virtual Kalapana](#virtual-kalapana)  
[Flux - SRPO](#flux-srpo)  
[Virtual Paris](#virtual-paris)  
[Adventures](#adventures)  
[Receipt](#receipt)  

---

[Main Page](../index.md)

<hr style="height:12px;border-width:0;color:pink;background-color:pink">









## Virtual Kalapana

**19.Sep.2025**   
A lot of the improvement has been in model development. Flux has fixed a lot of issues with distorted, missing or excess anatomy although it still happens. What really helps is patience: you simply fix the prompt and re-queue. And modifying the prompt again, and re-queueing. So, I was hoping for Kalapana back in the day. Is it? no way. Feels more like MacKenzie Park with palm trees instead of ironwoods. And those coconuts... really?

<!-- Note to self: do not indent markdown!!  /-->

<div class="flex-container" markdown>
<div class="column" markdown>

<img src="/assets/images/emily/08-Flux1D25.jpg" alt="Girl at Kalapana" style="width: 420px;"/>

[ Girl at Kalapana 01 ]
</div>

<div class="column column-right" markdown>

Here's the prompt:  
*This photograph captures a solitary beautiful woman walking towards the viewer on a dark, volcanic black sand beach. She is positioned slightly off-center to the right, facing towards the camera. The woman has long brown hair and wears a black one-piece lycra swimsuit and is carrying a surfboard under her arm. She walks barefoot towards the viewer, leaving small footprints in the wet sand near the shoreline.*  
*The background features a dense cluster of tall, lush palm trees with green fronds and some golden-brown leaves, indicating the tropical location. Under the trees are a few small fallen coconuts, dead coconut tree fronds and black sand, NO shrubs or bushes. The sky above is clear and dark blue, suggesting it is morning with good weather conditions. The black sand contrasts sharply with the white surf at the water's edge on the left side of the image*.  
*The overall composition emphasizes the natural beauty of the beach setting, combining the dark volcanic sand with the vibrant greenery and tall palm trees in the background. The photograph uses bright, vivid colors to enhance the tropical atmosphere and highlights the solitude of the woman amidst the serene environment*.

</div>
</div>

---

<div class="flex-container" markdown>
<div class="column" markdown>

<img src="/assets/images/emily/09-Flux1D33.jpg" alt="Girl at Kalapana" style="width: 420px;" />

[ Girl at Kalapana 02 ]

</div>
  
<div class="column column-right" markdown>

<img src="/assets/images/emily/10-Flux1D34.jpg" alt="Girl at Kalapana" style="width: 420px;" />

[ Girl at Kalapana 03 ]

</div>
</div>

---

The prompt is a lot more verbose (wordy). You can add a lot of descriptors. However, it all comes down to what the model wants to do. And will it obey? At times yes, at times no. As they say: YMMV (your mileage may vary).

These are sequential 'queues': one is walking towards, the other: away. I didn't ask for "walking away". I was able to get the coconuts smaller. The bathing suit is a more current style than what the style was back in the day.

Issues? Definitely. What's with the hand at the tip of the bard on the image on the left? And the girl's shadow on the right suggests that the board is no longer intact. Overall, getting there but still a ways to go.

<hr style="height:4px;border-width:0;color:pink;background-color:pink">





## Flux SRPO

**21.Sep.2025**   
So, I wrote the text, and created the images, for the girl on the beach example on *Friday* in the model Flux.1 Dev.


<img src="/assets/images/emily/12-SRPOGirl12.jpg" alt="Girl at Kalapana" style="float: right; width: 450px;
        margin-left: 20px; margin-bottom: 10px;" />


Today is *Sunday*... and a new model has 'dropped': SRPO, which stands for 'Semantic Relative Preference Optimization'. Yep, conveys just about as much information to *my* mind, too.

One of the issues that seems to be at play here is source-of-image. In order to create an image, there has to be something to reference: for example, to be able to create cat images, the developers had poured 'tons and tons' (figuratively-speaking) of cat images into the model, which then, during the image-generation process, gets referenced so the KSampler knows what the prompt is asking for. I've actually created my own 'models' called LoRAs (low ranking adapters) where I create a character based on a set of images of that character at different angles and situations, which images, when paired with relevant text, creates a LoRA through a rather long, tedious process. I can then do a face-swap with what the original model came up with with my character's face (and body, if my LoRA contains that information). This is the process the alarmist media call 'deep-fake', except I use it for consistent character imagery, without which illustrations for stories would be confusing, indeed.

So, nothing about the image to the right is real: nothing. Indeed, the surfboard seems to be delaminating: that would suck. I didn't ask for a delaminating surfboard: I just got given it. Thanks, SRPO. ðŸ¤¨

Still, overall, the image is an improvement over what I see Flux1.Dev produce. It's still not Kalapana, more like the beach at the bottom of Waipio Valley. But, no question it is an improvement. At least the figure isn't walking away. 

So, there's that.

Here are some of the better of subsequent queues. Simply click the thumbnail to see a larger version.

<style>
  .flex-container {display: flex; gap: 20px;}
  .column {width: 100%;}
</style>


<div class="flex-container">
  <div class="column">

  <a href="/assets/images/emily/13-SRPOGirl17.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/13-SRPOGirl17.jpg" alt="Go" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/14-SRPOGirl20.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/14-SRPOGirl20.jpg" alt="Go" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/15-SRPOGirl21.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/15-SRPOGirl21.jpg" alt="Go" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/16-SRPOGirl22.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/16-SRPOGirl22.jpg" alt="Go" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/17-SRPOGirl23.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/17-SRPOGirl23.jpg" alt="Go" width="100" loading="lazy"></a>

  </div>
</div>


Feels like I've done enough Kalapana for now.

<hr style="height:4px;border-width:0;color:pink;background-color:pink">





## Virtual Paris

**22.Sep.2025**   
So, we're going from Hawaii to Paris. Because: why not. 


<img src="/assets/images/emily/24-Paris01.jpg" alt="Paris" style="float: right; width: 350px;
        margin-left: 20px; margin-bottom: 10px;"/>


So, for the past months I was a total [Flux.1 Dev](https://flux1ai.com/dev) fan: the images it produced were, in terms of accuracy and believability, so far superior to what went before, like Stable-Diffusion 1x and even SDXL, that it made those models *almost* irrelevant. Almost, because I would still use SDXL models for image-to-image (i2i) projects.

Oh, that takes me back.

I started AI-imaging for illustrations in Automatic1111 (A1111) using Stable Diffusion 1.5. Then, along came SDXL and SD 1.5 just sort-of ended up on the skids, as in: I never used it anymore. The base 1.5 models were over 4.5 gig in size, but one could find what were called 'Merge' models that offered improvements and were only a 2.2 gig download. 

Yes, gig. Gigabyte. Which is why you really need a decent internet connection and heaps of hard-drive space. We have fibre to the premises, fortunately. Also, you want to go with SSDs, not HDDs (spinning disc tech).

The original SDXL models weighed in at over 12 gig, but then, no one used the originals: everyone ran 'Merge' models that produced specialised art styles: wanna do Manga? there's an SDXL model for that. Archie-cartoon? yep, can help you with that. All in a 6.6 gig package. Realism was... okay - you just keep re-queueing if there was anything wonky.

One ends up discarding a lot of images, even if they were better than what SD 1.5 had produced. And now the game has changed dramatically, with Flux and SRPO (which is based on Flux). Most images are fine with no really hideous deformities: you decide what to keep based on prompt-adherence.

---


<img src="/assets/images/emily/25-Paris02.jpg" alt="Paris" style="float: left; width: 350px;
        margin-right: 20px; margin-bottom: 10px;"/>


SDXL was my go-to model for ages. To be honest, at the time the improvements over SD 1.5 *were* incredible. By now, AI-imaging had made Poser and Blender as tools for illustrations *completely irrelevant*. You type a few words (the prompt) and run queues until you get your desired result. Those results made truly **EVERYTHING** I had done in 3D software laughably poor.

Here's the thing: once you have that [that rather expensive graphics card](https://www.umart.com.au/pc-parts/computer-parts/graphics-cards-gpu-610?brand=5-61-313&mystock=1-6-7&filter_attr=0.120940-179291-115184.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0), there's nothing else to purchase. Well, except for SDDs... I did have to invest in storage. When I was doing images in Poser (and even Blender) I would suddenly **need** certain 'assets': clothing and hair for your figures, sets for the scenes, etc, which weren't expensive, but it added up after a while. 

None of that is even a thing now. The models are all a free download. Tutorials are available on YouTube.

Of course, nothing is perfect in life. AI art is no different. In this SRPO-based image, hands are great, hair is great, clothing is good, houses and cars and... wait, what about that bicycle? Something about it is... odd.

The instructions (the prompt) for these images was actually created by another AI tool: a large-language model called '[llama-joycaption-beta-one hf-llama:Q4_K_M](https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF)'. This incredible model can 'look' at a picture and "describe" it. In other words, it can turn an image into a prompt. 

That's just ... **crazy!!** - **it will create a prompt for you!** The approach I have been taking, then, is to either 'google' or 'pinterest' for a specific image containing the general setting I'm after. Of course, I could just type out a prompt, but LLMs generally do a much better job at this - they know what another LLM is looking for. If it misses the mark (introducing elements I don't want or just plain get the description wrong, which does happen), just run the query again. And again and again until it's close.

I'll sometimes make a few manual changes to the text of the prompt and run a few more queues. Each queue (with the SRPO model) now takes about 5 minutes or so. I usually run at least two or three before I do any adjustments to the prompt and re-run the queue. Rinse and repeat. You can see how time would fly by. The secret sauce is in the prompt: get that right, and you're 'home-n-hosed'.


<img src="/assets/images/emily/26-Paris03.jpg" alt="Paris" style="float: right; width: 350px;
        margin-left: 20px; margin-bottom: 10px;"/>


For these images, the prompt reads as follows...

- This photograph captures a young woman standing on an Parisian street corner at a Metro entrance, leaning against a black metal railing with bicycles parked beside her. She has brown hair in a loose chignon and is wearing a dark navy blue overcoat, gray turtleneck sweater, and light blue skinny jeans. One hand rests on wrought-iron railing while she holds a smartphone to her ear with the other hand.

- The background features a row of classic Parisian-style buildings with beige facades, tall windows, and wrought iron balconies. A red and black "METRO" sign is prominently visible in the upper center of the image. The sky above is overcast, creating a muted light that casts soft shadows on the scene. The street below has a crosswalk and some distant pedestrians. The overall color palette includes neutral tones from the woman's clothing and buildings, contrasted by the bright red Metro sign and bicycles. The photo has a slightly desaturated, almost painterly quality due to the selective use of color and lighting, giving it a timeless urban feel. The composition directs focus toward the woman while incorporating the surrounding cityscape elements.

---

In a few short years, we've gone from [this](https://www.reddit.com/r/StableDiffusion/comments/144clxb/beginner_here_why_are_closeup_photos_of/) to [this](https://www.reddit.com/r/comfyui/comments/1nx5dzd/night_drive_cat/). That's pretty wild.

<hr style="height:4px;border-width:0;color:pink;background-color:pink">









## Adventures

**25.Sep.2025**   
You might have noticed that the person in these images (and indeed in the 'Kalapana' images as well) all seem to be of the same person. When you do illustrations, having a persistent character is desirable. At this point, doing single-figure persistent characters is almost a no-brainer.

My next challenge: having not only two or more persistent figures, but have them interact. Watch this space. 

In the meantime, adventures suggest themselves.


<div class="flex-container">
  <div class="column">
  <a href="/assets/images/emily/27-Distress48.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/27-Distress48.jpg" alt="Where" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/30-Destiny13.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/30-Destiny13.jpg" alt="Do" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/29-Boxcar08.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/29-Boxcar08.jpg" alt="We" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/30-Destiny12.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/30-Destiny12.jpg" alt="Go" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/31-Destiny23.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/31-Destiny23.jpg" alt="Go" width="100" loading="lazy"></a>

  <a href="/assets/images/emily/32-Destiny20.jpg" target="_blank" rel="noopener">
  <img src="/assets/images/emily/32-Destiny20.jpg" alt="Go" width="100" loading="lazy"></a>

  </div>
</div>

<hr style="height:4px;border-width:0;color:pink;background-color:pink">








## Receipt

**30.Sep.2025**   
Martin's "letter" arrived today. My observations are [documented here](../family/index.md/#30-sep-2025).

<hr style="height:4px;border-width:0;color:pink;background-color:pink">









<!--
[![Where]()](/assets/images/emily/27-Distress48.jpg)
{ target="_blank" rel="noopener" }
/-->

<hr style="height:4px;border-width:0;color:pink;background-color:pink">



<!-- NEXT ENTRY HERE!!!  /-->

<hr style="height:16px;border-width:0;color:pink;background-color:pink">

[Main Page](../index.md)
