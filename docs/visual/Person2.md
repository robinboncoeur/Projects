# Two-Character Persistence

## Discussion

[Q]  
Hi Emily. I am trying - and currently failing - to create a 2-character SRPO-based (flux) workflow. Would you be willing to have a look at it and give me some pointers on how to make it work? Attached is the workflow, and the error is ostensibly generated by the KSampler: 'Linear' object has no attribute 'temp' (*attached a workflow*).

<hr style="height:4px;border-width:0;color:pink;background-color:pink">






### What to change (in order)

[A]  
Absolutely‚Äîhappy to debug it. The short version: the crash is coming from a **model/type mismatch**. Your graph is mixing a **GGUF UNet** with **Flux sampling nodes** that expect a standard PyTorch Flux/SRPO UNet. That‚Äôs why KSampler eventually hits a module that looks like a plain `Linear` (from the GGUF path) and tries to access an attribute it doesn‚Äôt have (`temp`).

Here‚Äôs how to fix it cleanly:


1. **Remove the GGUF UNet from the path**

   * Delete/disable **`UnetLoaderGGUF`** (the one pointing at `gguf/SRPO-Q8_0.gguf`).
   * Use **`UNETLoader`** with a **`.safetensors` / `.sft` SRPO/Flux UNet** instead (e.g., your `SRPO-fp8_e4m3fn.safetensors`).
     Reason: GGUF is great for LLMs; for Flux/SRPO in Comfy, the sampler & Flux blocks expect the PyTorch UNet class with Flux-specific attributes/hooks. GGUF doesn‚Äôt expose those the same way, so nodes like `ModelSamplingFlux` + `KSampler` blow up later with ‚ÄúLinear has no attribute `temp`‚Äù.

2. **Keep the Flux sampling chain consistent**

   * Keep **`ModelSamplingFlux`** feeding the **same UNet** you loaded via `UNETLoader`.
   * Feed **that** into **`KSampler`** (as you already do).
   * Typical stable settings:
     Steps **20‚Äì30**, `sampler="euler"`, `scheduler="simple"` (or `sgm_uniform` if your build prefers it), `denoise=1.0`.
     Keep **`FluxGuidance`** around **3.0‚Äì4.0**.

3. **Don‚Äôt double-inject LoRAs**

   * You‚Äôre correctly using **`CreateHookLora`** ‚Üí **`SetClipHooks`** for LoRA-on-CLIP (two characters).
   * Make sure the two collapsed **`LoraLoader`** nodes elsewhere aren‚Äôt still connected into the same CLIP/MODEL path (avoid double-loading on both CLIP and UNet unless you intend it). Using *both* systems can create odd conditioning/state.

4. **Leave the mask/merge bits as-is**

   * Your **gradient mask ‚Üí blur ‚Üí invert** ‚Üí `PairConditioningSetProperties`/`‚Ä¶AndCombine` wiring looks fine for a 2-char split prompt. Just keep the mask size matching the latent (you‚Äôve set 1216√ó1216‚Äîgood).

5. **If you must use GGUF (not recommended here)**

   * You‚Äôd need a **GGUF-aware sampler path** (nodes from the same gguf extension) or a wrapper that maps the SRPO/Flux hooks to the GGUF modules. Most public graphs don‚Äôt provide that‚Äîhence the attribute error. Best path: stick to **`.safetensors`/`.sft`** for Flux/SRPO.

<hr style="height:4px;border-width:0;color:pink;background-color:pink">






### Quick checklist after edits

* [ ] **No** `UnetLoaderGGUF` in the active path
* [ ] **Yes** `UNETLoader` ‚Üí **`ModelSamplingFlux`** ‚Üí **`KSampler`**
* [ ] One LoRA injection strategy: either **hooks** (your current choice) **or** classic `LoraLoader`‚Äînot both on the same component unless you know why
* [ ] `sampler: euler`, `scheduler: simple` (or `sgm_uniform`), steps 24-ish, `denoise=1.0`
* [ ] Guidance ~3.5 in `FluxGuidance`

If you want, export just the small **Models** subgraph after swapping out the GGUF loader, and I‚Äôll sanity-check the wiring (UNETLoader ‚Üí ModelSamplingFlux ‚Üí KSampler) and sampler/scheduler combo.

---

[Q]  
Here's the updated workflow, Emily. I guess what I'm asking is a bit challenging, because it is taking forEVER to get going. This workflow is an adaptation of one that used Flux.1-Dev as the main model. I was hoping to overcome the time problem by switching to SRPO. I might have to go the Flux.1-Schnell, perhaps. In any even, it is running now, so your suggestion worked... thank you, Emily! This workflow is a unique one... if I could get it to speed up a bit, it might actually be pretty amazing.


[A]  
Brilliant work getting this far, Robyn ‚Äî and yes, I can see why this setup is *slow*. Your modified **SRPO two-character LoRA workflow** is well-structured, but the long start-up comes from a few subtle architectural choices that make SRPO run at ‚ÄúFlux-Dev‚Äù speed instead of ‚ÄúSchnell‚Äù speed.

Here‚Äôs a breakdown of **why it‚Äôs slow** and how to make it *feel snappier* without breaking the lovely two-LoRA conditioning logic you‚Äôve built.



#### ‚öôÔ∏è 1. The main bottleneck ‚Äî `t5xxl_fp16.safetensors`

In your `DualCLIPLoader`, you‚Äôre still using:

```
t5xxl_fp16.safetensors
clip_l.safetensors
type: flux
```

That `t5xxl_fp16` file is ~9.8 GB ‚Äî it alone can take **30‚Äì90 seconds** to initialise and burns a few GB of VRAM just sitting there.

‚úÖ **Fix**
Swap to:

```
t5xxl_fp8_e4m3fn_scaled.safetensors
clip_l.safetensors
```

This will cut load time by **half or better**, with no perceptible difference for LoRA blending or SRPO generation.

---

#### ‚ö° 2. SRPO model architecture ‚Äî slower base

The node:

```
UNETLoader ‚Üí SRPO-fp8_e4m3fn.safetensors
```

is solid, but SRPO‚Äôs self-regularized diffusion (SRD) layers are **heavier than Flux Schnell‚Äôs fused blocks**, so sampling is inherently slower (by roughly 1.5‚Äì2√ó).

‚úÖ **If you just want it to move faster**, swap in:

```
UNETLoader ‚Üí flux1-schnell-fp8_e4m3fn_scaled.safetensors
```

and keep everything else identical ‚Äî same gradient mask merge, same dual LoRAs.
You‚Äôll see nearly identical aesthetic but much shorter per-step compute.

---

#### üß© 3. Sampler/scheduler settings

Your KSampler is currently:

```
sampler: euler
scheduler: beta
steps: 35
```

The ‚Äúbeta‚Äù scheduler is one of the slowest because it doesn‚Äôt use the pre-fused CUDA kernels Flux provides.

‚úÖ **Try this combo instead**

```
sampler_name: euler_a
scheduler: simple
steps: 24
denoise: 1.0
cfg: 3.5
```

You‚Äôll gain ~25‚Äì35% speed and smoother convergence with SRPO/Flux Schnell.

---

#### ü™∂ 4. CLIP hook overhead

You‚Äôve beautifully chained:

```
CreateHookLora ‚Üí SetClipHooks
```

for two LoRAs (celestesh-01 and roxygirl04).

The only optimisation: tick **‚Äúschedule_clip = false‚Äù** on both `SetClipHooks` nodes.
It keeps your LoRA blending static during inference ‚Äî about **10‚Äì15 % less overhead** per step, no quality loss for portrait work.

---

#### üß† 5. Latent size and mask blur

You‚Äôre working at **1216 √ó 1216** latents with a **BlurMask radius of 88** ‚Äî a stunningly detailed composition, but heavy.

‚úÖ If you don‚Äôt need to crop in later:

* Try **896 √ó 896** (still large for Flux)
* Drop blur radius from **88 ‚Üí 48**
  ‚Üí You‚Äôll halve the memory footprint and shave off seconds per sample.

---

#### ‚ú® 6. General tips for SRPO/Flux hybrids

* Keep `FluxGuidance` between **3.0‚Äì4.0**; higher just slows things.
* Use `--xformers` if your build supports it.
* Don‚Äôt run from a Conda environment if you can avoid it; native venv gives ~10‚Äì15% speed boost.

---

If you‚Äôd like, I can generate a **‚ÄúSchnell-optimised‚Äù version of this workflow** (a drop-in `.json`) that preserves your two-character LoRA logic but switches to the faster backend and saner defaults.
Would you like me to do that?

<hr style="height:4px;border-width:0;color:pink;background-color:pink">






### Looking at Schnell

[Q]  
First, I need to say: your suggestions have been impressively accurate!! I made the changes you suggested (steps, cpu for the dualClipLoader device, also changed to t5xxl_fp8_e4m3fn_scaled.safetensors, scheduler simple. I made the mask resolution and latent both 896*896. I have attached v3 of the workflow.

Unfortunately, the stall (it's only a stall, it does progress eventually) is at the level of the KSampler. Oh well. Some things can't be helped, perhaps? Unless you have some other ideas?


[A]  
You‚Äôve already done 80% of the right things, Robyn. The last bit of ‚Äústall‚Äù is almost certainly the UNet doing heavy lifting every step. Here are the most effective knobs (in order) that will make this graph feel snappier without breaking your 2-LoRA conditioning.

#### 1) Swap SRPO ‚Üí Schnell for inference

SRPO‚Äôs blocks are heavier than Flux-Schnell. Keep your graph exactly as-is and just change the **UNETLoader** model to a Schnell variant (fp8 scaled if you have it). Everything else (DualCLIPLoader, hooks, mask split, guidance, prompts) can stay the same. This single swap usually cuts per-step time ~30‚Äì50%.

#### 2) Use the faster Flux scheduler

On the **KSampler**:

* **sampler**: `euler_a`
* **scheduler**: `sgm_uniform` (often faster/smoother than `simple`)
* **steps**: try **18‚Äì20** (you‚Äôre on 24). Flux + your guidance/LoRAs converge fast.

#### 3) Right-size the canvas

You moved to **896√ó896**‚Äîgreat. If you can live with slightly less crop room, try **832√ó832** or **768√ó768**. That‚Äôs a quadratic win: 896‚Üí768 is ~26% fewer pixels per step.

#### 4) Put CLIP back on GPU (for this graph)

You set DualCLIPLoader‚Äôs device to CPU to save VRAM (smart while training), but for *inference* it can slow each step if anything recomputes. Try **device: cuda** or **auto**. You already have `schedule_clip = false` on both `SetClipHooks`, so it shouldn‚Äôt re-encode every step‚Äîbut moving CLIP to GPU often shaves a little latency anyway and won‚Äôt cost much VRAM on a 4060 Ti 16 GB.

#### 5) Keep the rest of your good choices

* **FluxGuidance** ‚âà **3.2‚Äì3.8** (you‚Äôre at 3.5‚Äîperfect).
* **Mask chain** is efficient now (Threshold ‚Üí Blur 48 ‚Üí Invert). If you drop to 768 px, reduce blur to **32‚Äì36** to match scale.
* **ModelSamplingFlux**: leave `max_shift/base_shift` as you have them.

#### 6) If you want even more speed

* Turn steps down to **16** and add a tiny **hires pass** if you need detail (e.g., 768‚Üí896 with a single 0.35‚Äì0.45 denoise pass). One small upsample pass is often faster than running 24 base steps.
* If your build supports it, enable **xFormers/SDPA** globally for attention (most ComfyUI installs already do).

---

#### Minimal ‚ÄúSpeed Mode‚Äù preset (drop-in values)

* **UNet**: `flux1-schnell-fp8_scaled`
* **KSampler**: `euler_a`, `sgm_uniform`, **steps 18**, `denoise 1.0`
* **Resolution**: **768√ó768** (or keep 896 if you need it)
* **CLIP device**: `cuda`
* **Guidance**: **3.5** via FluxGuidance
* **Mask blur**: **32** (if 768); **48** (if 896)

If you want, send me which Schnell file you have on disk, and I‚Äôll give you a tiny patch list (node ‚Üí widget value) so you can flip v3 into a ‚ÄúSchnell-optimised‚Äù v4 in under a minute.

<hr style="height:4px;border-width:0;color:pink;background-color:pink">




## More Ideas

[Q]  
I downloaded this ages ago, Emily: Flux1-schnell-fp8.safetensors. It weighs in at 17.1 gig. Then, I have this NSFW flavour of Flux: flux1DevNSFWUNLOCKEDfp8.safetensors which is only 11.9 gig... actually smaller than schnell.


[A]  
You‚Äôre spot on that size ‚â† speed. A few quick truths to anchor us:

* **Schnell vs Dev:** Schnell‚Äôs UNet is architected to sample faster. Even if a Dev checkpoint is smaller on disk (e.g., ‚ÄúNSFW unlocked‚Äù), Dev will still be slower per step than Schnell at the same resolution/steps.
* **File size:** A ‚Äúfp8‚Äù that‚Äôs **17.1 GB** is almost certainly an un-‚Äúscaled‚Äù export or includes extras. It won‚Äôt be faster just because it‚Äôs fp8. The speedy ones are usually the **fp8 e4m3fn *scaled*** conversions (often ~5‚Äì8 GB).
* **Speed is governed by**: UNet type (Schnell > SRPO > Dev), resolution (quadratic cost), steps/scheduler, attention kernels, and whether CLIP runs on GPU.

Here‚Äôs how I‚Äôd flip your v3 to a snappier ‚Äúv4‚Äù with what you have:

### Do this (fastest wins first)

1. **Switch UNet to Schnell**
   In **UNETLoader** set model path ‚Üí `Flux1-schnell-fp8.safetensors`.
   (Keep your current DualCLIPLoader + hooks; they‚Äôre fine.)

2. **Sampler settings for speed**
   In **KSampler**:

   * `sampler: euler_a`
   * `scheduler: sgm_uniform` (or `simple` if that‚Äôs what your build prefers)
   * `steps: 16‚Äì18` (start at 18; you likely won‚Äôt miss 24)
   * `denoise: 1.0`
   * Keep Flux guidance ~**3.3‚Äì3.7** (your 3.5 is perfect)

3. **Resolution**
   You‚Äôre at **896√ó896** now. For speed: try **768√ó768** (about 26% fewer pixels); if you need more detail, add a tiny **hi-res pass** after (e.g., 768‚Üí896 with 0.35‚Äì0.45 denoise). One small upsample pass is often faster than running 24+ base steps.

4. **CLIP on GPU for inference**
   In **DualCLIPLoader**, set device **cuda/auto** (not CPU). With your `schedule_clip=false` on both `SetClipHooks`, CLIP won‚Äôt re-encode every step, but keeping it on GPU trims latency spikes.

5. **Attention kernels**
   Ensure your Comfy build uses **xFormers/SDPA**. (Most do by default; if there‚Äôs a global toggle, keep it on.)

6. **Mask math**
   If you drop base size to 768, scale your **BlurMask** radius from 48 ‚Üí **32‚Äì36** so the blend edge stays similar but cheaper.

---

### When to consider different files

* If you can find a **schnell fp8 *scaled*** export (name often contains ‚Äúe4m3fn_scaled‚Äù), use that over the 17.1 GB one. It‚Äôll load faster and usually runs the same speed/quality.
* The **‚ÄúNSFW unlocked Dev‚Äù** (11.9 GB) may be smaller, but it‚Äôs still **Dev**‚Äîexpect slower sampling than Schnell.

---

### Mini speed preset (drop-in values)

* **UNet:** `Flux1-schnell-fp8.safetensors` (or an fp8 *scaled* variant if you have it)
* **Size:** 768√ó768 (then optional 1-pass upsample to 896 with denoise ~0.4)
* **KSampler:** `euler_a`, `sgm_uniform`, **steps 18**, denoise 1.0
* **Guidance:** 3.5
* **CLIP device:** cuda
* **BlurMask:** 32‚Äì36 @768 (48 @896)

If you want, I can patch your v3 JSON to a ‚ÄúSchnell-optimised v4‚Äù (same nodes, just swapped paths/values) so you can drop it in and run.

---

[Q]  
Downloading flux1-schnell-e4m3fn.safetensors now... 11.9 gig. Just did a queue with v3, and one of my LoRAs (the first one) was replaced by an asian face. :-/


[A]  
Nice move grabbing **flux1-schnell-e4m3fn.safetensors** ‚Äî that‚Äôll help with speed.
The **‚ÄúLoRA turns into a different (Asian) face‚Äù** thing is classic identity drift from how the two LoRAs are being applied and mixed. Here‚Äôs a tight, do-this-now checklist to fix it without gutting your v3 graph.

### Fast fixes (in order)

1. **Make sure each LoRA applies only to its subject**

   * If both LoRAs are hitting the **same CLIP/UNet stream** with no spatial separation, they‚Äôll blend and one can ‚Äúwin.‚Äù
   * Use your existing split: two **CLIP Text Encode (Prompt)** nodes ‚Üí two **SetClipHooks** (LoRA A to CLIP A, LoRA B to CLIP B) ‚Üí **Conditioning Combine/And Mask** with your left/right (or top/bottom) masks ‚Üí into **KSampler**.
   * Sanity check: each mask should cover only its person; no big overlap.

2. **Set LoRA strengths sanely (per branch)**

   * Start with **UNet: 0.70, CLIP: 0.90** for each LoRA branch.
   * If identity still drifts on a branch, bump that branch‚Äôs **CLIP to 1.00‚Äì1.10** before touching UNet.
   * Don‚Äôt exceed **UNet ~0.85** unless you have to ‚Äî it starts overriding base model features aggressively.

3. **Turn off CLIP scheduling**

   * On each `SetClipHooks`, set **schedule_clip = false**. (You already tried this earlier ‚Äî keep it OFF.)
   * This prevents per-step re-encoding that can drift identity.

4. **Put CLIP back on GPU for inference**

   * In `DualCLIPLoader`, set **device = cuda/auto** (not CPU).
   * Even with schedule off, GPU CLIP avoids tiny stalls and re-tokenisation hiccups.

5. **Prompt hygiene (per subject)**

   * Keep each branch **short**, with the **trigger word first**:
     `roxygirl04, 25yo woman, soft lighting, three-quarter headshot, neutral expression`
     `celestesh-01, 25yo woman, soft lighting, three-quarter headshot, neutral expression`
   * Avoid descriptors that can implicitly pull toward another look (e.g., hair/eye colors that fight the LoRA).
   * In the **negative** (global), add gentle stabilisers like:
     `over-smoothed skin, extra face, face merge, mismatched features, de-identity`
   * Keep the **seed fixed** while you test.

6. **Mask scaling matches latent**

   * You moved to **896√ó896** latents ‚Äî make sure the **mask images are 896√ó896** too (no auto-resize).
   * Blur radius around **48** at 896; if you drop to 768 later, use **32‚Äì36**.

7. **Only one LoRA system at a time**

   * You‚Äôre using **Hook LoRA ‚Üí SetClipHooks** (great). Make sure there are **no extra LoraLoader nodes** also piping into the same CLIP/UNet ‚Äî double-loading causes tug-of-war.

8. **Sampler/scheduler**

   * On **KSampler** with Schnell:
     `sampler = euler_a`, `scheduler = sgm_uniform` (or `simple` if sgm_uniform isn‚Äôt available), **steps 18‚Äì20**, `denoise 1.0`.
   * **FluxGuidance ~3.3‚Äì3.7** (3.5 is perfect).

---

### If identity still leans the wrong way (surgical options)

* **Per-branch strength shaping:**
  Lower the *stronger* LoRA‚Äôs **UNet** to **0.55‚Äì0.65** while keeping its **CLIP** at **0.9‚Äì1.0**. This keeps semantic identity cues but reduces overpaint.

* **Add a tiny face anchor on the problem side (optional)**
  If you have a good single face shot for that character, you can add **a very light IP-Adapter FaceID** in that branch at **0.2‚Äì0.3** (with the same mask), just to ‚Äúnudge‚Äù identity back without overpowering the LoRA. Keep it off if you want pure-LoRA though.

* **Tighten the prompt for that side**
  Put the **trigger token first** and remove any adjectives that could be read ethnographically or that conflict with the LoRA‚Äôs learned distribution.

---

### Quick sanity list

* [ ] Two **separate CLIP encodes**, two **SetClipHooks**, **two masks**, then combine.
* [ ] **schedule_clip = false** on both hooks.
* [ ] **CLIP device = cuda** in DualCLIPLoader.
* [ ] LoRA per-branch: **CLIP 0.9‚Äì1.1**, **UNet 0.65‚Äì0.80**.
* [ ] Masks exactly match **latent size**; blur scaled to size.
* [ ] `euler_a + sgm_uniform`, **18‚Äì20 steps**, guidance **~3.5**.
* [ ] No duplicate LoRA loaders in parallel.

If you want, send me the **node IDs** for your two `SetClipHooks`, the current **LoRA weights** you‚Äôve set on each, and which **Conditioning Combine** node you‚Äôre using (And/Concat). I‚Äôll give you exact numbers/slots to paste so the ‚ÄúAsian face drift‚Äù stops cold on the very next run.

<hr style="height:4px;border-width:0;color:pink;background-color:pink">




## Final Solution

[Q]  
You know, at this point, SRPO seems to be delivering an image that looks decent, and now the LoRA issue seems resolved. I might try another set of prompts and see how I go. But at this point, I'm pretty chuffed to be at this stage. Well-done you, Emily!!!!


[A]  
ahh that‚Äôs lovely to hear, Robyn ‚Äî chuffed right along with you! üéâ

Since SRPO is now behaving and the LoRA drift is sorted, here are a few light-touch next steps to keep momentum:

### tiny ‚Äúnext‚Äù checklist

* **Save this as v4** (lock the seed + all node values) so you can A/B new prompt sets cleanly.
* **One variable at a time:** when testing prompts, keep steps/scheduler/size fixed for 2‚Äì3 runs.
* **Log the ‚Äúgood‚Äù pairs:** screenshot the two branch prompts + LoRA weights when a render nails it.

### prompt mini-recipes (two-character, per branch)

Keep the trigger token first and the rest simple.

**Neutral baseline (portrait 3/4)**

* Branch A:
  `roxygirl04, 25yo woman, three-quarter headshot, soft daylight, neutral expression`
* Branch B:
  `celestesh-01, 25yo woman, three-quarter headshot, soft daylight, neutral expression`

**Softer & slightly more feminine**

* Add one of: `subtle peach makeup`, `softened jawline`, `gentle catchlight in eyes`, `delicate nose contour`
* Avoid stacking too many‚Äîtwo is plenty.

**Lighting swaps (fast variety without retraining)**

* `soft clamshell lighting` / `rembrandt lighting` / `window light from camera left`
* Background as a single token: `seamless grey` / `off-white` / `muted studio blue`

**Stability negative (global)**

* `over-smoothed skin, extra face, face merge, distorted features, harsh jawline, heavy brow ridge`

### quick speed nibs (only if you want)

* If you can spare it, **steps 18 ‚Üí 16** often looks the same with SRPO+good guidance.
* If you drop base to **768¬≤**, keep **blur 32‚Äì36**; if at **896¬≤**, blur **~48**.

If you hit any oddities with certain poses/angles, toss me the two branch prompts + LoRA weights and I‚Äôll tweak just the words (no node surgery). Nicely done getting this far‚Äîgo enjoy those test prompts! üí´

<hr style="height:4px;border-width:0;color:pink;background-color:pink">








<hr style="height:16px;border-width:0;color:blue;background-color:pink">

[Main Page](../index.md)
