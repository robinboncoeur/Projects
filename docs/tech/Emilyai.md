# Emily

## AI — Useful? 

Short answer: yes. And fun and, to some degree, addictive.

I don't 'Google' anymore, I go straight to 'Emily', my OpenAI ChatGPT 5 bot with all the questions I have about, sheesh, anything! 

Hostinger, the ISP hosting my website tightbytes.com was going to charge me over AUD$500 for a two-year thing, for a site **no one visits**, and that I'm basically using as a remote file server. So I asked Emily what she would suggest... and which suggestion I'm actually following now. So, now the pages live on my github site—FREE—and are displayed by Cloudflare—also FREE—so the only thing I pay for is video hosting, which is $5 a month. Not only am I using the approach Emily suggested, but she's also helping me get it all set up! The information she provides is all online: she just pulls it all together WAY better than Google ever could.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">






## Imaging using Words

### 2025 September

Here is an example of what is now—2025—possible with the technology. These videos are about the music for a story I'm writing, bit of an explore, that one:

<style>
  .columnA {float: left; padding-right: 20px;}
  .left {width: 40%;}
  .right {width: 60%;}
</style>

<div class="row">
  <div class="columnA">
    <video width="384" height="384" controls>
      <source src="https://www.tightbytes.com/music/Sketches/Sketch15.mp4" type="video/mp4">
    </video>
    <br>
    [Version Française]
  </div>
  <div class="columnB">
    <video width="384" height="384" controls>
      <source src="https://www.tightbytes.com/music/Sketches/Sketch15EN.mp4" type="video/mp4">
    </video>
    <br>
    [English Version]

  </div>
</div>

There are multiple technologies at play here: text-to-image, image-to-video, text-to-audio and audio+image-to-video... and that's just for the visual side of things. I wrote the music in [Musescore Studio](https://musescore.org/en) using the [MuseSounds](https://www.musehub.com/muse-sounds) library. The crazy bit: all of this can be done on any PC, as long as it has a decent (read: *expensive*) graphics card. At home, not on some expensive remote server: I'm using the power of our solar cells. Is the process truly green? an argument can be made for "not really". Emily is on ChatGPT, so those computations are not done on my system at home.

There are things I won't use AI for, like music composition. Could I? Sure. Much of the pop/hip-hop/rock genre you hear on the radio is at least partly AI-generated. But, like when painting with oils, a non-human source would quickly betray itself. Sure, I'll use AI to study how to write music better, but the creative process remains completely mine. 

Just like writing, AI-generated images (and music) is quickly identifiable. In human likenesses generated by older models, the person's face is often distorted, or the figure ends up with 3 fingers, or six. The newer, more recent models are more accurate, but there's always a 'tell', a quickly-identifiable charachteristic betraying AI's "finger in the pie". There's an AI company in Germany called "Black Forest Labs" that has put together this amazing image-generation model called 'Flux'. I use it pretty much all the time: 9 times out of 10, the fingers and toes and background are correct, 'realistic'.

Anyway, a bit of history: how I got into this.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">







### History

<img src="/assets/images/emily/00-Charlotte.jpg" alt="Midjourney" style="float: right; width: 300px;
        margin-right: 20px; margin-bottom: 10px;" />
		
When I first started messing with image generation (to illustrate writings, but also just mucking about) I used [Midjourney](https://www.midjourney.com/explore?tab=video_top). This was in the days when AI-video wasn't a thing: it was just about images. You type in a prompt—you had to follow a certain formula for that prompt—and you made some images.

There were limits as to how many images one could create based on subscription, so I switched to [LeonardoAI](https://leonardo.ai/), a cheaper option. It wasn't long before I ran into the image count limit.

And then, I heard about[ Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion), something I could run on my own PC. Unlimited images, do-your-own-thing. Best thing was: no app/program to run... you run it in your browser, like Chrome of Firefox, using the [Automatic1111](https://en.wikipedia.org/wiki/Automatic1111) interface. Sure, it was a bit meh after a while, much-of-a-muchness, poor control of image quality... but at least there was no limit on images.

And then [ComfyUI](https://www.comfy.org/gallery) appeared on the scene.

I've pretty-much stayed with [ComfyUI](https://www.comfy.org/about) ever since. A bit about the outfit behind ComfyUI, from their FAQ:

---

Q: Is ComfyUI free?
- Yes! ComfyUI is 100% free and open-source — and always will be.

Q: Who is behind ComfyUI?
- ComfyUI is built and maintained by the original core team, organized as an independent company. We’re here to keep it evolving for the long term.

Q: How can I contribute?
- Join our Discord, contribute on GitHub, build custom nodes, or share your workflows!

Q: What are your future plans?
- We aim to grow ComfyUI while staying true to our open, flexible, community-driven philosophy.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">






## 2025 AI Exploring

### September 19

ComfyUI is an interface to make images and video that runs in the browser, like A1111. However, the control you have is infinitely more granular. To make it work requires:

<img src="/assets/images/emily/02-4060ti-16gig.jpg" alt="NVidia graphics card" style="float: right; width: 200px;
        margin-right: 20px; margin-bottom: 10px;" />

- decent graphics card (6-8 gig VRAM NVidia minimum, 16-24 gig VRAM NVidia preferable)
- models: Stable Diffusion 1x / 1.5x / SDXL / SD3; Flux1 (Dev/Snell); other models exist
- decent internet access: those models can run up to 24 gig

I have a 16 gig VRAM 4060ti NVidia card... which was, for me, **expensive**: AUD $750.00!! An 8 gig card will still set you back a good AUD $350.00.  
However, the rest of the PC is older, inexpensive tech. You don't need an expensive processor (CPU). Saying that, I recently invested in 64 gig RAM and an older i7 CPU in order to run Large Language Models (LLMs). This is text-based work, like ChatGPT, but running all on your own PC. I was using it to translate Dad's book. *To be honest, though, I've gone back to ChatGPT 5 for that project*.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">






### September 20

The language for AI to 'generate' images and video and text is called a "prompt". Prompting is very much a model-specific thing: I "talk" to Emily in a completely different manner, using different sentence structure and syntax, than the language I use in a prompt for an image. And even prompting for that... imaging... has changed over the past years, and even months. 

A Midjourney prompt would include formatting-specific instructions formatted a certain way. 

Stable Diffusion and SDXL need to be told what **NOT** to show as well as what to show and there are special boxes for each.

<img src="/assets/images/emily/03-Screenshot.jpg" alt="Basic workflow" style="width: 800px;"/>

The ComfyUI interface is made up of 'nodes': each node performs a specific function and is connected to other nodes using 'noodles'. For example, the 'Load Checkpoint' node loads the imaging model, which in this case is 'Realistic Vision version 5.1'. There are literally **thousands** of imaging models out there, downloadable for free from a variety of websites, the best of which is [HuggingFace](https://huggingface.co/models).

The green and 'red' nodes are the text encode prompts: 'green' for what I **want** to see, and 'red' for what I **DO NOT** want to see.

The empty latent image node is a bit difficult to explain... it is basically random 'noise' which is essentially what gets turned into a viewable image by the KSampler, which is sort-of the heart of the thing. It uses the model, the positive and negative prompts and the latent image (as well as other bits-n-bobs) to create an image:

<img src="/assets/images/emily/04-ComfyUI_1.jpg" alt="Purple Galaxy"/>

Yay!

So, how is this useful? Well, paradoxically, the reason I wanted to be able to do this is to prevent copyright-infringement issues when using images to illustrate a book, or a webpage or whatever. *Incidently, it's why I started writing my own music as well, so I'd have licence-free music for my videos.* I want to be able to create illustrations easily and quickly for whatever I need it for. I needed an image of a waterfall for the cover page of my little piece [Waterfall](https://musescore.com/user/29275325/scores/20905957). Yep: AI came through. Oh, I generated almost a dozen before I had something I liked.

Illustrations for a story is a bit trickier. So, a bit of background...

I wanted to write a period piece, set in the Victorian era or before. The images (referred to as 'queues' in ComfyUI, I guess because it's assumed you're going to be generating images one after the other until you get something you like) I created in A1111 were pretty much all rubbish, or extremely poor. I've deleted them now: not worth the hard-disk space. *BTW, doing the same thing with all my Poser stuff: it's embarassing now how awful that stuff was*!

Early days in ComfyUI quickly got complicated. Here's a workflow from then:

<img src="/assets/images/emily/05-Screenshot.jpg" alt="Workflow"/>

and the prompt:

<img src="/assets/images/emily/06-Screenshot.jpg" alt="Positive prompt"/>

to get this:

<img src="/assets/images/emily/07-ComfyUI_3.jpg" alt="Girl at a cafe" style="width: 450px;"/>
																			 
So, wonky left leg, right hand has 3 fingers, nonsensical text on signs... and where the heck is she sitting? on the sidewalk?!? Also, what about that necklace?

Yeah, not perfect. But the tech very, very quickly improved. ComfyUI itself dramatically improved. Everything has become better, faster, more accurate, more flexible offering more possibilities... and it is all free. Yes: no-cost. Once you have purchased your graphics card, you're good to go.



I would like to highlight what **ELSE** can be done with this process: image repair. Here's an original image I found online of Kalapana (the way I remember it, actual **Kaimu**)... an image with scratches, artifact blotches, a really poor photo of a photo:

<img src="/assets/images/emily/18-KalapanaO.jpg" alt="Kaimu Beach" style="width: 850px;"/>

...and here is how it looks, AI-fixed:

<img src="/assets/images/emily/19-KalapanaF.jpg" alt="Kaimu Beach" style="width: 850px;"/>

So, there's that. 

---

Helps to know how to tell the model what to do... it's all in the 'prompt'. I wanted to restore an old, rather poor photograph of Alice Mary Smith for my transcription of her work. Here's my prompt:

- *Restore this damaged vintage portrait by removing scratches and stains, then add realistic period-appropriate colors, including existing dress textures. Maintain the same facial features of the blonde young woman and enhance the texture of white-dotted silk fabric of her dress*.

I did have to tweak the prompt a bit, as you do. I used this technique for a cover image for my transcription of [her "Lalla Rookh"](https://musescore.com/user/29275325/scores/26630581)... the original picture was, let's say, detail-poor:

<img src="/assets/images/emily/20-AliceMaryO.jpg" alt="AMSmith" style="width: 450px;"/>

...which Flux1 Kontext 'imagined' to this:

<img src="/assets/images/emily/21-AliceMaryF.jpg" alt="AMSmith" style="width: 700px;"/>

Just from this perspective alone, the effort is worth it. Just like transcribing [this marvelous work of Alice's,](https://musescore.com/user/29275325/scores/26630581) putting in a little extra effort makes it so worth it. Is this Alice? Who knows. Perhaps a newer, better, more accurate model will do better. Until then, I'm pretty happy.

By the way, sometimes it helps to change the sampler and scheduler. Her dress had these dots on it in the original picture, and the 'deis' sampler and 'kl_optimal' scheduler wasn't doing the dress right. So, I went with bog-standard euler sampler and ddim_uniform scheduler to get this:

<img src="/assets/images/emily/22-AliceMaryF2.jpg" alt="AMSmith" style="width: 700px;"/>

Are we having fun, yet? Changing sampler and scheduler gave Alice a bit of lippie (Aussie for 'lipstick') but yeah, very similar outcome, from a facial feature viewpoint. 

Just for shits-n-giggles, tried the SRPO on this... and ended up with a massive **FAIL**! As in, a completely blurry horrid blob of nothing recognisable. Conclusion: **SRPO** is **not** an image-to-image tool, like Flux Kontext.

Whilst mucking around with all this, I'm listening to [Serenade #4 by Robert Fuchs](https://www.youtube.com/watch?v=l8ERA4HnxRc). That last movement really puts the weasel in your gut.

Oh, by the way, there are these modified versions of models that come under the 'gguf' umbrella. What is a 'gguf'? It is a binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes. From a practical perspective, it's a more usable version of a model for someone who doesn't have a graphics card with insane amounts of VRAM. My graphics card has 16gig of VRAM. Respectable. Decent amount. But there are cards out the that cost **tens of thouands of dollars** that have, um, more. LOTS more.

The last Flux1 Kontext model was a GGUF model. It was an 8-bit model. 'Bit' is a term about quantization. What is 'quantization'? Well, Quantization is the process of mapping continuous infinite values to a smaller set of discrete finite values. In the context of simulation and embedded computing, it is about approximating real-world values with a digital representation that introduces limits on the precision and range of a value.

Short answer: it's about accuracy, detail and approximation. 8-bit approximates quite a bit better than 4-bit or 2-bit. Still, whilst the 8-bit model created that last image, a 4-bit model created this:

<img src="/assets/images/emily/23-AliceMaryF3.jpg" alt="AMSmith" style="width: 700px;"/>

Not a shabby effort. Not as stellar as 8-bit but still heaps better than the original. Who know which one is the most accurate. Not from that time period, so it's likely to remain anyone's guess, for now.

---

Note: *I'm going to continue documenting progress in ComfyUI [on my blog](../other/Blog25.md/#exploring-comfyui) on this site.*

See [Blog 25][b25] for other things I'm up to.



<hr style="height:12px;border-width:0;color:blue;background-color:blue">

Note to self: **Quick Links**

[b25]: ../other/Blog25.md
