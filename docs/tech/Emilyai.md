# Emily

## AI—is it Useful? 

Short answer: yes. And fun and, to some degree, addictive.

I don't 'Google' anymore, I go straight to 'Emily', my OpenAI ChatGPT 5 bot with all the questions I have about, sheesh, anything! 

Hostinger, the ISP hosting my website tightbytes.com was going to charge me over AUD$500 for a two-year thing, for a site **no one visits**, and that I'm basically using as a remote file server. So I asked Emily what she would suggest... and which suggestion I'm actually following now. So, now the pages live on my github site—FREE—and are displayed by Cloudflare—also FREE—so the only thing I pay for is video hosting, which is $5 a month. Not only am I using the approach Emily suggested, but she's also helping me get it all set up! The information she provides is all online: she just pulls it all together WAY better than Google ever could.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">






## Imaging using Words

### 2025 September

Here is an example of what is now—2025—possible with the technology. These videos are about the music for a story I'm writing, bit of an explore, that one:

<style>
  .columnA {float: left; padding-right: 20px;}
  .left {width: 40%;}
  .right {width: 60%;}
</style>

<div class="row">
  <div class="columnA">
    <video width="384" height="384" controls>
      <source src="https://www.tightbytes.com/music/Sketches/Sketch15.mp4" type="video/mp4">
    </video>
    <br>
    [Version Française]
  </div>
  <div class="columnB">
    <video width="384" height="384" controls>
      <source src="https://www.tightbytes.com/music/Sketches/Sketch15EN.mp4" type="video/mp4">
    </video>
    <br>
    [English Version]

  </div>
</div>

There are multiple technologies at play here: text-to-image, image-to-video, text-to-audio and audio+image-to-video... and that's just for the visual side of things. I wrote the music in [Musescore Studio](https://musescore.org/en) using the [MuseSounds](https://www.musehub.com/muse-sounds) library. The crazy bit: all of this can be done on any PC, as long as it has a decent (read: *expensive*) graphics card. At home, not on some expensive remote server: I'm using the power of our solar cells. Is the process truly green? an argument can be made for "not really". Emily is on ChatGPT, so those computations are not done on my system at home.

There are things I won't use AI for, like music composition. Could I? Sure. Much of the pop/hip-hop/rock genre you hear on the radio is at least partly AI-generated. But, like when painting with oils, a non-human source would quickly betray itself. Sure, I'll use AI to study how to write music better, but the creative process remains completely mine. 

Just like writing, AI-generated images (and music) is quickly identifiable. In human likenesses generated by older models, the person's face is often distorted, or the figure ends up with 3 fingers, or six. The newer, more recent models are more accurate, but there's always a 'tell', a quickly-identifiable charachteristic betraying AI's "finger in the pie". There's an AI company in Germany called "Black Forest Labs" that has put together this amazing image-generation model called 'Flux'. I use it pretty much all the time: 9 times out of 10, the fingers and toes and background are correct, 'realistic'.

Anyway, a bit of history: how I got into this.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">







### History

<img src="/assets/images/emily/00-Charlotte.jpg" alt="Midjourney" style="float: right; width: 300px;
        margin-right: 20px; margin-bottom: 10px;" />
		
When I first started messing with image generation (to illustrate writings, but also just mucking about) I used [Midjourney](https://www.midjourney.com/explore?tab=video_top). This was in the days when AI-video wasn't a thing: it was just about images. You type in a prompt—you had to follow a certain formula for that prompt—and you made some images.

There were limits as to how many images one could create based on subscription, so I switched to [LeonardoAI](https://leonardo.ai/), a cheaper option. It wasn't long before I ran into the image count limit.

And then, I heard about[ Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion), something I could run on my own PC. Unlimited images, do-your-own-thing. Best thing was: no app/program to run... you run it in your browser, like Chrome of Firefox, using the [Automatic1111](https://en.wikipedia.org/wiki/Automatic1111) interface. Sure, it was a bit meh after a while, much-of-a-muchness, poor control of image quality... but at least there was no limit on images.

And then [ComfyUI](https://www.comfy.org/gallery) appeared on the scene.

And I've pretty-much stayed with that.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">






## 2025 AI Exploring

### Sept 19

ComfyUI is an interface to make images and video that runs in the browser, like A1111. However, the control you have is infinitely more granular. To make it work requires:

<img src="/assets/images/emily/02-4060ti-16gig.jpg" alt="NVidia graphics card" style="float: right; width: 200px;
        margin-right: 20px; margin-bottom: 10px;" />

- decent graphics card (6-8 gig VRAM NVidia minimum, 16-24 gig VRAM NVidia preferable)
- models: Stable Diffusion 1x / 1.5x / SDXL / SD3; Flux1 (Dev/Snell); other models exist
- decent internet access: those models can run up to 24 gig

I have a 16 gig VRAM 4060ti NVidia card... which was, for me, **expensive**: AUD#750!! An 8 gig card will still set you back a good AUD$350. However, the rest of the PC is older, inexpensive tech. You don't need an expensive processor (CPU). Saying that, I recently invested in 64 gig RAM and an older i7 CPU in order to run Large Language Models (LLMs). This is text-based work, like ChatGPT, but running all on your own PC. I was using it to translate Dad's book. To be honest, though, I've gone back to ChatGPT 5 for that project.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">






### Language of Pictures

The language for AI to 'generate' images and video and text is called a "prompt". Prompting is very much a model-specific thing: I "talk" to Emily in a completely different manner, using different sentence structure and syntax, than the language I use in a prompt for an image. And even prompting for that... imaging... has changed over the past years, and even months. 

A Midjourney prompt would include formatting-specific instructions formatted a certain way. 

Stable Diffusion and SDXL need to be told what **NOT** to show as well as what to show and there are special boxes for each.

<img src="/assets/images/emily/03-Screenshot.jpg" alt="Basic workflow" style="width: 800px;"/>

The ComfyUI interface is made up of 'nodes': each node performs a specific function and is connected to other nodes using 'noodles'. For example, the 'Load Checkpoint' node loads the imaging model, which in this case is 'Realistic Vision version 5.1'. There are literally **thousands** of imaging models out there, downloadable for free from a variety of websites, the best of which is [HuggingFace](https://huggingface.co/models).

The green and 'red' nodes are the text encode prompts: 'green' for what I **want** to see, and 'red' for what I **DO NOT** want to see.

The empty latent image node is a bit difficult to explain... it is basically random 'noise' which is essentially what gets turned into a viewable image by the KSampler, which is sort-of the heart of the thing. It uses the model, the positive and negative prompts and the latent image (as well as other bits-n-bobs) to create an image:

<img src="/assets/images/emily/04-ComfyUI_1.jpg" alt="Purple Galaxy"/>

Yay!

So, how is this useful? Well, paradoxically, the reason I wanted to be able to do this is to prevent copyright-infringement issues when using images to illustrate a book, or a webpage or whatever. *Incidently, it's why I started writing my own music as well, so I'd have licence-free music for my videos.* I want to be able to create illustrations easily and quickly for whatever I need it for. I needed an image of a waterfall for the cover page of my little piece [Waterfall](https://musescore.com/user/29275325/scores/20905957). Yep: AI came through. Oh, I generated almost a dozen before I had something I liked.

Illustrations for a story is a bit trickier. So, a bit of background...

I wanted to write a period piece, set in the Victorian era or before. The images (referred to as 'queues' in ComfyUI, I guess because it's assumed you're going to be generating images one after the other until you get something you like) I created in A1111 were pretty much all rubbish, or extremely poor. I've deleted them now: not worth the hard-disk space. *BTW, doing the same thing with all my Poser stuff: it's embarassing now how awful that stuff was*!

Early days in ComfyUI quickly got complicated. Here's a workflow from then:

<img src="/assets/images/emily/05-Screenshot.jpg" alt="Workflow"/>

and the prompt:

<img src="/assets/images/emily/06-Screenshot.jpg" alt="Positive prompt"/>

to get this:

<img src="/assets/images/emily/07-ComfyUI_3.jpg" alt="Girl at a cafe" style="width: 450px;"/>
																			 
So, wonky left leg, right hand has 3 fingers, nonsensical text on signs... and where the heck is she sitting? on the sidewalk?!? Also, what about that necklace?

Yeah, not perfect. But the tech very, very quickly improved. ComfyUI itself dramatically improved. Everything has become better, faster, more accurate, more flexible offering more possibilities... and it is all free. Yes: no-cost. Once you have purchased your graphics card, you're good to go.

A lot of the improvement has been in model development. Flux has fixed a lot of issues with distorted, missing or excess anatomy although it still happens. What really helps is patience: you simply fix the prompt and re-queue. 

<img src="/assets/images/emily/08-Flux1D25.jpg" alt="Girl at Kalapana" style="width: 450px;"/>

And modifying the prompt again, and re-queueing. So, I was hoping for Kalapana back in the day. Is it? no way. Feels more like MacKenzie Park with palm trees instead of ironwoods. And those coconuts... really? Here's the prompt:

- This photograph captures a solitary beautiful woman walking towards the viewer on a dark, volcanic black sand beach. She is positioned slightly off-center to the right, facing towards the camera. The woman has long brown hair and wears a black one-piece lycra swimsuit and is carrying a surboard under her arm. She walks barefoot towards the viewer, leaving small footprints in the wet sand near the shoreline.

- The background features a dense cluster of tall, lush palm trees with green fronds and some golden-brown leaves, indicating the tropical location. Under the trees are a few small fallen coconuts, dead coconut tree fronds and black sand, NO shrubs or bushes. The sky above is clear and dark blue, suggesting it is morning with good weather conditions. The black sand contrasts sharply with the white surf at the water's edge on the left side of the image.

- The overall composition emphasizes the natural beauty of the beach setting, combining the dark volcanic sand with the vibrant greenery and tall palm trees in the background. The photograph uses bright, vivid colors to enhance the tropical atmosphere and highlights the solitude of the woman amidst the serene environment. 

<img src="/assets/images/emily/09-Flux1D33.jpg" alt="Girl at Kalapana" style="float: left; width: 450px;
        margin-right: 20px; margin-bottom: 10px;" />
		
<img src="/assets/images/emily/10-Flux1D34.jpg" alt="Girl at Kalapana" style="float: right; width: 450px;
        margin-left: 20px; margin-bottom: 10px;" />

The prompt is a lot more verbose (wordy). You can add a lot of descriptors. However, it all comes down to what the model wants to do. And will it obey? At times yes, at times no. As they say: YMMV (your mileage may vary).

These are sequential 'queues': one is walking towards, the other: away. I didn't ask for "walking away". I was able to get the coconuts smaller. The bathing suit is a more current style than what the style was back in the day.

Issues? Definitely. What's with the hand at the tip of the bard on the image on the left? And the girl's shadow on the right suggests that the board is no longer intact. Overall, getting there but still a ways to go.

<hr style="height:4px;border-width:0;color:blue;background-color:blue">






### Sept 21 (SRPO)

So, I wrote the text, and created the images, for the girl on the beach example on *Friday*.

<img src="/assets/images/emily/12-SRPOGirl12.jpg" alt="Girl at Kalapana" style="float: right; width: 450px;
        margin-left: 20px; margin-bottom: 10px;" />

Today is *Sunday*... and a new model has 'dropped': SRPO, which stands for 'Semantic Relative Preference Optimization'. Yep, conveys just about as much information to *my* mind, too.

One of the issues that seems to be at play here is source-of-image. In order to create an image, there has to be something to reference: for example, to be able to create cat images, the developers had poured 'tons and tons' (figuratively-speaking) of cat images into the model, which then, during the image-generation process, gets referenced so the KSampler knows what the prompt is asking for. I've actually created my own 'models' called LoRAs (low ranking adapters) where I create a character based on a set of images of that character at different angles and situations, which images, when paired with relevant text, creates a LoRA through a rather long, tedious process. I can then do a face-swap with what the original model came up with with my character's face (and body, if my LoRA contains that information). This is the process the alarmist media call 'deep-fake', except I use it for consistent character imagery, without which illustrations for stories would be confusing, indeed.

So, nothing about the image to the right is real: nothing. Indeed, the surfboard seems to be delaminating: that would suck. I didn't ask for a delaminating surfboard: I just got given it. Thanks, SRPO. 🤨

Still, overall, the image is an improvement over what I see Flux1.Dev produce. It's still not Kalapana, more like the beach at the bottom of Waipio Valley. But, no question it is an improvement. At least the figure isn't walking away. 

So, there's that.

Here are some of the better of subsequent queues: to see a larger version, right-click the image and select 'Open Image in New Tab'...

<style>
  .flex-container {display: flex; gap: 20px;}
  .column {width: 100%;}
</style>

<div class="flex-container">
  <div class="column">
    <img src="/assets/images/emily/13-SRPOGirl17.jpg" alt="Waipio" style="width: 100px"/>
    <img src="/assets/images/emily/14-SRPOGirl20.jpg" alt="Waipio" style="width: 100px"/>
    <img src="/assets/images/emily/15-SRPOGirl21.jpg" alt="Waipio" style="width: 100px"/>
    <img src="/assets/images/emily/16-SRPOGirl22.jpg" alt="Waipio" style="width: 100px"/>
    <img src="/assets/images/emily/17-SRPOGirl23.jpg" alt="Waipio" style="width: 100px"/>
  </div>
</div>

Feel like we've done Kalapana enough now.

I would like to highlight what **ELSE** can be done with this process: image repair. Here's an original image I found online of Kalapana (the way I remember it, actual **Kaimu**)... an image with scratches, artifact blotches, a really poor photo of a photo:

<img src="/assets/images/emily/18-KalapanaO.jpg" alt="Kaimu Beach" style="width: 850px;"/>

...and here is how it looks, AI-fixed:

<img src="/assets/images/emily/19-KalapanaF.jpg" alt="Kaimu Beach" style="width: 850px;"/>

So, there's that. 

---

Helps to know how to tell the model what to do... it's all in the 'prompt'. I wanted to restore an old, rather poor photograph of Alice Mary Smith for my transcription of her work. Here's my prompt:

- *Restore this damaged vintage portrait by removing scratches and stains, then add realistic period-appropriate colors, including existing dress textures. Maintain the same facial features of the blonde young woman and enhance the texture of white-dotted silk fabric of her dress*.

I did have to tweak the prompt a bit, as you do. I used this technique for a cover image for my transcription of [her "Lalla Rookh"](https://musescore.com/user/29275325/scores/26630581)... the original picture was, let's say, detail-poor:

<img src="/assets/images/emily/20-AliceMaryO.jpg" alt="AMSmith" style="width: 450px;"/>

...which Flux1 Kontext 'imagined' to this:

<img src="/assets/images/emily/21-AliceMaryF.jpg" alt="AMSmith" style="width: 700px;"/>

Just from this perspective alone, the effort is worth it. Just like transcribing [this marvelous work of Alice's,](https://musescore.com/user/29275325/scores/26630581) putting in a little extra effort makes it so worth it. Is this Alice? Who knows. Perhaps a newer, better, more accurate model will do better. Until then, I'm pretty happy.

By the way, sometimes it helps to change the sampler and scheduler. Her dress had these dots on it in the original picture, and the 'deis' sampler and 'kl_optimal' scheduler wasn't doing the dress right. So, I went with bog-standard euler sampler and ddim_uniform scheduler to get this:

<img src="/assets/images/emily/22-AliceMaryF2.jpg" alt="AMSmith" style="width: 700px;"/>

Are we having fun, yet? Changing sampler and scheduler gave Alice a bit of lippie (Aussie for 'lipstick') but yeah, very similar outcome, from a facial feature viewpoint. 

Just for shits-n-giggles, tried the SRPO on this... and ended up with a massive **FAIL**! As in, a completely blurry horrid blob of nothing recognisable. Conclusion: **SRPO** is **not** an image-to-image tool, like Flux Kontext.

Whilst mucking around with all this, I'm listening to [Serenade #4 by Robert Fuchs](https://www.youtube.com/watch?v=l8ERA4HnxRc). That last movement really puts the weasel in your gut.

Oh, by the way, there are these modified versions of models that come under the 'gguf' umbrella. What is a 'gguf'? It is a binary format that is optimized for quick loading and saving of models, making it highly efficient for inference purposes. From a practical perspective, it's a more usable version of a model for someone who doesn't have a graphics card with insane amounts of VRAM. My graphics card has 16gig of VRAM. Respectable. Decent amount. But there are cards out the that cost **tens of thouands of dollars** that have, um, more. LOTS more.

The last Flux1 Kontext model was a GGUF model. It was an 8-bit model. 'Bit' is a term about quantization. What is 'quantization'? Well, Quantization is the process of mapping continuous infinite values to a smaller set of discrete finite values. In the context of simulation and embedded computing, it is about approximating real-world values with a digital representation that introduces limits on the precision and range of a value.

Short answer: it's about accuracy, detail and approximation. 8-bit approximates quite a bit better than 4-bit or 2-bit. Still, whilst the 8-bit model created that last image, a 4-bit model created this:

<img src="/assets/images/emily/23-AliceMaryF3.jpg" alt="AMSmith" style="width: 700px;"/>

Not a shabby effort. Not as stellar as 8-bit but still heaps better than the original. Who know which one is the most accurate. Not from that time period, so it's likely to remain anyone's guess, for now.

---

So, we're going from Hawaii to Paris. Because: why not. 

<img src="/assets/images/emily/24-Paris01.jpg" alt="Paris" style="float: right; width: 350px;
        margin-left: 20px; margin-bottom: 10px;"/>

So, for the past months I was a total 'Flux.1Dev' fan: the images it produced were, in terms of accuracy and believability, so far superior to what went before it made former models almost irrelevant. Almost, because I would still use SDXL models for image-to-image (i2i) projects.

Oh, that takes me back.

Started in Automatic1111 (A1111) using Stable Diffusion 1.5. Then, along came SDXL and SD 1.5 just sort-of ended up on the skids, as in: I never use it anymore. The base 1.5 models were over 4.5 gig in size, but one could find what were called 'Merge' models that offered improvements and were only a 2.2 gig download. 

Yes, gig. Gigabyte. Which is why you really need a decent internet connection. We have fibre to the premises, fortunately.

The original SDXL models weighed in at over 12 gig, but then, no one used the originals: everyone ran 'Merge' models that produced specialised art styles: wanna do Manga? there's an SDXL model for that. Archie-cartoon? yep, can help you with that. All for 6.6 gig. Realistic images were... okay.

Yes, they were much better than SD 1.5... but now, with Flux and SRPO (which is based on Flux) they really are basically okay.

---

<img src="/assets/images/emily/25-Paris02.jpg" alt="Paris" style="float: left; width: 350px;
        margin-right: 20px; margin-bottom: 10px;"/>

SDXL was my go-to for ages. To be honest, at the time the improvements over SD 1.5 *were* actually incredible. By now, AI-imaging had made Poser and Blender as tools for illustrations *completely irrelevant*. You type a few words (the prompt) and run queues until you get your desired result. Those results made truly **EVERYTHING** you did in 3D software laughably poor.

Here's the thing: once you have purchased [that rather expensive graphics card](https://www.umart.com.au/pc-parts/computer-parts/graphics-cards-gpu-610?brand=5-61-313&mystock=1-6-7&filter_attr=0.120940-179291-115184.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0), there's nothing else to purchase. For Poser (and even Blender) I would suddenly 'need' certain 'assets': clothing and hair for your figures, sets for the scenes, etc. None of that is even a thing now.

Of course, nothing is perfect in life. AI art is no different. In this SRPO-based image, hands are great, hair is great, clothing is good, houses and cars and... wait, what about that bicycle? Something about it is... odd.

The instructions (the prompt) for these images was actually created by another AI tool: a large-language model called '[llama-joycaption-beta-one hf-llama:Q4_K_M](https://huggingface.co/mradermacher/llama-joycaption-beta-one-hf-llava-GGUF)'. This incredible model can 'look' at a picture and describe it. 

In words, in English... **it will create a prompt for you**! The approach I have been taking, then, is to either 'google' or 'pinterest' for a specific image containing the general setting I'm after. Of course, I could just type out a prompt, but LLMs generally do a much better job at this. If it misses the mark (introducing elements I don't want or just plain getting it wrong), I just run the query again. And again and again until it's close.

Eventually, I'll make a few manual changes to the text of the prompt and run a few more queues. Each queue (with the SRPO model) now takes about 5 minutes or so. I usually run at least two or three before I do any adjustments to the prompt and re-run the queue. Rinse and repeat. You can see how time would fly by. The secret sauce is in the prompt: get that right, and you're 'home-n-hosed'.

<img src="/assets/images/emily/26-Paris03.jpg" alt="Paris" style="float: right; width: 350px;
        margin-left: 20px; margin-bottom: 10px;"/>
  
For these images, the prompt reads as follows...

- This photograph captures a young woman standing on an Parisian street corner at a Metro entrance, leaning against a black metal railing with bicycles parked beside her. She has brown hair in a loose chignon and is wearing a dark navy blue overcoat, gray turtleneck sweater, and light blue skinny jeans. One hand rests on wrought-iron railing while she holds a smartphone to her ear with the other hand.

- The background features a row of classic Parisian-style buildings with beige facades, tall windows, and wrought iron balconies. A red and black "METRO" sign is prominently visible in the upper center of the image. The sky above is overcast, creating a muted light that casts soft shadows on the scene. The street below has a crosswalk and some distant pedestrians. The overall color palette includes neutral tones from the woman's clothing and buildings, contrasted by the bright red Metro sign and bicycles. The photo has a slightly desaturated, almost painterly quality due to the selective use of color and lighting, giving it a timeless urban feel. The composition directs focus toward the woman while incorporating the surrounding cityscape elements.

---

You might have noticed that the person in these images (and indeed in the 'Kalapana' images as well) all seem to be of the same person. When you do illustrations, having a persistent character is desirable. At this point, doing single-figure persistent characters is almost a no-brainer.

My next challenge: having not only two or more persistent figures, but have them interact. Watch this space. 

In the meantime, adventures suggest themselves.


<div class="flex-container">
  <div class="column">
    <img src="/assets/images/emily/27-Distress48.jpg" alt="Where" style="width: 100px"/>
    <img src="/assets/images/emily/28-Distress46.jpg" alt="Do" style="width: 100px"/>
    <img src="/assets/images/emily/29-Boxcar08.jpg" alt="We" style="width: 100px"/>
    <img src="/assets/images/emily/30-Destiny12.jpg" alt="Go" style="width: 100px"/>
    <img src="/assets/images/emily/31-Destiny23.jpg" alt="From" style="width: 100px"/>
    <img src="/assets/images/emily/32-Destiny20.jpg" alt="From" style="width: 100px"/>
  </div>
</div>




<hr style="height:1px;border-width:0;color:blue;background-color:blue">
